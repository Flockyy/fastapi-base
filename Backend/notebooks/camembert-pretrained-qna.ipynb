{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9755532145500183,\n",
       " 'start': 0,\n",
       " 'end': 15,\n",
       " 'answer': 'Emmanuel Macron'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp({\n",
    "  'question': 'Qui est le président de la France ?',\n",
    "  'context': 'Emmanuel Macron est le président de la France.'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers as st\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROCESSED_DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mResults are saved to models/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/evalu_results.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 65\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Load train, validation sets\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     train, valid \u001b[39m=\u001b[39m load_datasets()\n\u001b[0;32m     27\u001b[0m     \u001b[39m# Initialize model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcamembert\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mload_datasets\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_datasets\u001b[39m():\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Load train, validation sets\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     train_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PROCESSED_DATA_PATH, \u001b[39m'\u001b[39m\u001b[39mtrain.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     val_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PROCESSED_DATA_PATH, \u001b[39m'\u001b[39m\u001b[39mval.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(train_path, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PROCESSED_DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import simpletransformers as st\n",
    "import json \n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "from utils import format_squad, PROCESSED_DATA_PATH\n",
    "import os\n",
    "\n",
    "def load_datasets():\n",
    "    # Load train, validation sets\n",
    "    train_path = os.path.join(PROCESSED_DATA_PATH, 'train.json')\n",
    "    val_path = os.path.join(PROCESSED_DATA_PATH, 'val.json')\n",
    "\n",
    "    with open(train_path, encoding=\"utf-8\") as f:\n",
    "        train = json.load(f)\n",
    "\n",
    "    with open(val_path, encoding=\"utf-8\") as f:\n",
    "        valid = json.load(f)\n",
    "\n",
    "    return format_squad(train), format_squad(valid)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load train, validation sets\n",
    "    train, valid = load_datasets()\n",
    "\n",
    "    # Initialize model\n",
    "    model_type = \"camembert\"\n",
    "    model_name = \"camembert-base\"\n",
    "\n",
    "    train_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"use_cached_eval_features\": True,\n",
    "    \"output_dir\": f\"models/{model_name}\",\n",
    "    \"best_model_dir\": f\"models/{model_name}/best_model\",\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"num_train_epochs\": 10, # Takes too long to train ~3 hours\n",
    "    \"evaluate_during_training_steps\": 1000,\n",
    "    \"use_wandb\": False,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    \"save_eval_checkpoints\": False,\n",
    "    \"n_best_size\":3, # The total number of n-best predictions to generate in the nbest_predictions.json output file.\n",
    "    \"train_batch_size\": 16, # Reduced to 16 after getting CUDA out of memory error\n",
    "    \"eval_batch_size\": 16, # Reduced to 16 after getting CUDA out of memory error\n",
    "    }\n",
    "\n",
    "    # Create a QuestionAnsweringModel\n",
    "    model = QuestionAnsweringModel(\n",
    "    model_type,model_name, args=train_args, use_cuda=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train, eval_data=valid)\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, texts = model.eval_model(valid)\n",
    "    with open(f\"models/{model_name}/evalu_results.json\", 'w') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f'Results are saved to models/{model_name}/evalu_results.json')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
